{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安装torch\n",
    "1. 检查cuda版本: NVIDIA-smi\n",
    "2. 使用uv pip安装: `uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "else:\n",
    "    torch.set_default_device('cpu')\n",
    "\n",
    "x = torch.randn(3, 3)\n",
    "print(x.device)  # 如果有 GPU，则输出 `cuda:0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a, b):\n",
    "    return np.dot(a, b)\n",
    "\n",
    "# 使用torch\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(4, 2)\n",
    "c = a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matadd(a, b):\n",
    "    return a + b\n",
    "\n",
    "# 使用torch\n",
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transpose(a):\n",
    "    return a.T\n",
    "\n",
    "# useage\n",
    "transpose(np.array([[1, 2], [3, 4]]))\n",
    "\n",
    "# 或者使用torch\n",
    "a = torch.randn(3, 4)\n",
    "a_t = a.t()           # 转置为 4x3\n",
    "# 或交互两个维度\n",
    "a = torch.randn(2, 3, 4)\n",
    "a_t = torch.transpose(a, 1, 2)  # 交换第1和第2维度, 输出为 2x4x3, 第一个维度可以看作是batch_size, 没有变\n",
    "\n",
    "# 使用permute, permute可以一次性交换多个维度\n",
    "a = torch.randn(2, 3, 4)\n",
    "a_t = a.permute(0, 2, 1)  # 交换第1和第2维度, 输出为 2x4x3, 第一个维度可以看作是batch_size, 没有变\n",
    "torch.equal(a_t, torch.transpose(a, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(6, 2)\n",
    "b = a.reshape(3, 4)     # 重塑为 3x4\n",
    "# 或使用 view (需连续)\n",
    "b = a.view(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_matrix(shape):\n",
    "    return np.random.randn(*shape)\n",
    "\n",
    "# useage\n",
    "init_matrix((2, 2))\n",
    "\n",
    "\n",
    "# 使用torch\n",
    "zeros = torch.zeros(3, 4)       # 全零矩阵\n",
    "ones = torch.ones(3, 4)         # 全一矩阵\n",
    "rand_matrix = torch.rand(3, 4)  # 均匀分布 [0,1)\n",
    "randn_matrix = torch.randn(3, 4)# 标准正态分布\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc sin and cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 1.5708, 3.1416], device='cuda:0')\n",
      "sin(x): tensor([ 0.0000e+00,  1.0000e+00, -8.7423e-08], device='cuda:0')\n",
      "cos(x): tensor([ 1.0000e+00, -4.3711e-08, -1.0000e+00], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量\n",
    "x = torch.tensor([0.0, torch.pi / 2, torch.pi])\n",
    "print(x)\n",
    "\n",
    "# 计算正弦值\n",
    "sin_x = torch.sin(x)\n",
    "print(\"sin(x):\", sin_x)\n",
    "\n",
    "# 计算余弦值\n",
    "cos_x = torch.cos(x)\n",
    "print(\"cos(x):\", cos_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word tokenization(tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([64], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "data = \"a\"\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenized_text = encoding.encode(data)\n",
    "max_token_value = max(tokenized_text) + 1\n",
    "print(max_token_value)\n",
    "tokenized_text = torch.tensor(tokenized_text, dtype=torch.long)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get random index in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15339,  1917,     0,  3475,   828,   369,  4037,  2065,    13],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 3]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "data = \"hello world! Test data for tokenization.\"\n",
    "tokenized_text = encoding.encode(data)\n",
    "# tokenize后, 我们可以使用torch.tensor将其转换为张量\n",
    "tokenized_text = torch.tensor(tokenized_text, dtype=torch.long)\n",
    "print(tokenized_text)\n",
    "\n",
    "batch_size = 2\n",
    "context_length = 3\n",
    "\n",
    "# 生成一个随机的下标, 注意这里size是元组, 代表我们每个批次选一个开始下标\n",
    "idxs = torch.randint(low=0, high=len(tokenized_text) - context_length, size=(batch_size,))\n",
    "\n",
    "# 这样就可以继续从tokenized_texta中取出对应批次的数据, x_batch是输入, y_batch是预期输出\n",
    "x_batch = torch.stack([tokenized_text[idx:idx + context_length] for idx in idxs])\n",
    "y_batch = torch.stack([tokenized_text[idx + 1:idx + context_length + 1] for idx in idxs])\n",
    "print(x_batch.shape, x_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply matrix mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4496, 0.0780, 0.1184, 0.3280],\n",
      "        [0.2820, 0.1881, 0.1188, 0.3170],\n",
      "        [0.9326, 0.5730, 0.3664, 0.1999]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.rand(3, 4)\n",
    "print(a)\n",
    "\n",
    "mask = (a > 0.9)          # 生成布尔掩码\n",
    "masked_b = a.masked_fill(mask, -1e9)  # 将正值替换为 -1e9\n",
    "\n",
    "# 可视化\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(masked_b.cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3163, 0.9202, 0.6243, 0.6699],\n",
      "        [0.5641, 0.3972, 0.6005, 0.3296],\n",
      "        [0.2281, 0.3202, 0.3345, 0.2969]], device='cuda:0')\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True]], device='cuda:0')\n",
      "tensor([[ 3.1629e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "        [ 5.6409e-01,  3.9724e-01, -1.0000e+09, -1.0000e+09],\n",
      "        [ 2.2814e-01,  3.2016e-01,  3.3451e-01, -1.0000e+09]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 如果和矩阵的一部分mask为具体的值\n",
    "a = torch.rand(3, 4)\n",
    "print(a)\n",
    "\n",
    "# 生成一个上三角矩阵. diagonal为0, 代表包含对角线\n",
    "mask = torch.triu(torch.ones_like(a, dtype=torch.bool), diagonal=1)\n",
    "# mask = torch.triu(torch.ones(a.shape[0], a.shape[1], dtype=torch.bool), diagonal=1)\n",
    "print(mask)\n",
    "\n",
    "# a矩阵中mask为True的部分替换为-1e9\n",
    "masked_a = a.masked_fill(mask, value=-1e9)\n",
    "print(masked_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8580,  1.2356, -0.5407, -0.4103],\n",
      "        [-1.4469,  1.3955,  0.8138, -0.1618],\n",
      "        [ 0.1205,  0.6529,  1.5561, -0.3242]], device='cuda:0')\n",
      "tensor([[0.5777, 0.3100, 0.0525, 0.0598],\n",
      "        [0.0319, 0.5471, 0.3058, 0.1153],\n",
      "        [0.1325, 0.2257, 0.5568, 0.0849]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(3, 4)\n",
    "print(logits)\n",
    "softmax = torch.nn.functional.softmax(logits, dim=-1)  # 沿最后一维计算(每一行)\n",
    "print(softmax)\n",
    "sum(softmax[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc sqrt root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = nn.LayerNorm(2)\n",
    "# 把前面的输出作为输入, 调用layer_norm\n",
    "# layer_norm(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 维度\n",
    "        self.d_model = d_model\n",
    "        # 丢失率\n",
    "        self.dropout = dropout\n",
    "        # 两个线性层, 先从d_model映射到d_model*4, 再从d_model*4映射到d_model, 中间有ReLU激活函数, 最后有dropout\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_model, out_features=self.d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.d_model * 4, out_features=self.d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵损失函数`F.cross_entropy(input=logits, target=targets_reshaped)`期望输入的形状是 (N, C) 和 (N)，其中 N 是样本数量，C 是类别数量。\n",
    "> 我的理解: 为什么target里不需要考虑C(类别数), 因为target代表的意思就是预期的类别, 而我们的输入里放了C个类别的概率. 所以输入的每个序列里的某一个元素, 都有一个预期的类别, 这样target其实就相当于降维了.\n",
    "\n",
    "下面deepseek给了一个使用示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 假设我们有一个简单的分类任务\n",
    "# 输入是一个 batch 的 logits，目标是对应的类别标签\n",
    "\n",
    "# 定义 batch size (B), 类别数 (C), 序列长度 (T)\n",
    "B = 2  # batch size\n",
    "T = 3  # 序列长度(就是有几个词)\n",
    "C = 5  # 类别数（词汇表大小）\n",
    "\n",
    "# 随机生成 logits (模型的输出)\n",
    "# logits 的形状是 (B, T, C)\n",
    "logits = torch.randn(B, T, C)  # 随机生成 logits\n",
    "print(\"Logits (原始形状):\")\n",
    "print(logits)\n",
    "print(\"Logits 的形状:\", logits.shape)\n",
    "\n",
    "# 随机生成 targets (目标类别标签)\n",
    "# targets 的形状是 (B, T)\n",
    "targets = torch.randint(0, C, (B, T))  # 随机生成目标标签\n",
    "print(\"\\nTargets (原始形状):\")\n",
    "print(targets)\n",
    "print(\"Targets 的形状:\", targets.shape)\n",
    "\n",
    "# 将 logits 和 targets 重塑为 (B * T, C) 和 (B * T)\n",
    "logits_reshaped = logits.view(B * T, C)  # 重塑为 (B * T, C)\n",
    "targets_reshaped = targets.view(B * T)   # 重塑为 (B * T)\n",
    "print(\"\\nReshaped Logits (形状):\", logits_reshaped.shape)\n",
    "print(\"Reshaped Targets (形状):\", targets_reshaped.shape)\n",
    "\n",
    "# 使用 F.cross_entropy 计算损失\n",
    "loss = F.cross_entropy(logits_reshaped, targets_reshaped)\n",
    "print(\"\\n计算得到的损失值:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### how to use loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算出loss后, 我们就直到了初测出来的值和目标值的差异的大小, 这个时候借助反向传播, 我们就可以调整我们的参数, 使得我们的预测值更加接近目标值.\n",
    "\n",
    "涉及到的api:\n",
    "1. `loss.backward()`: 反向传播, 计算梯度\n",
    "2. `optimizer.step()`: optimizer会更新模型中的参数. 注意: 这个函数会更新所有的参数, 所以在调用这个函数之前, 我们需要先调用`optimizer.zero_grad()`清空梯度缓存."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于torch中contiguous的意思: https://stackoverflow.com/a/69599806/12855525\n",
    "1. 其实就是tensor中的数据是不是连续存储的，有没有出现跳跃的情况\n",
    "2. 如果tensor不是连续存储的，可以使用contiguous()方法来使其连续存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方差(mean), 公式: \n",
    "$μ = \\frac{1}{n}\\sum_{i=1}^{n}x_i$  \n",
    "标准差(standard deviation), 公式: \n",
    "$var = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2$\n",
    "$σ = \\sqrt{var}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer normalization的一些几何解释: [Re-Introducing LayerNorm: Geometric Meaning, Irreversibility and a Comparative Study with RMSNorm](https://arxiv.org/html/2409.12951v1#S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
