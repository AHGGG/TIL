### transformer最后输出逻辑与参数量

在经过多个block后, 会经过一个线性层, 这里也有一个权重矩阵Wp, 维度为d_model \* vocab_size. 前面的矩阵乘以这个Wp之后, 会得到和每个token预测的下一个token的概率分布.

尝试理解这里的矩阵乘法的逻辑:
1. 假设在线性层前面的矩阵是[16 \* 128], 也就是说有16个token, 每个token有128个维度
2. 线性层的**权重矩阵**是[128 \* vocab_size], 可以理解成每一列都代表了一个具体的token, 每一行: 代表token所在的某个维度. 这样权重矩阵中第一列[128, 1]的意思就是, 在当前上下文语境中, 这个token在128维度的空间中的具体位置, 然后分(投影)到每个维度都有一个具体的数值来表示
3. 这个时候[1, 128](代表一个128维的token) @ [128, vocab_size](代表有vocab_size个token, 每个token也是128维), 两个矩阵相乘, 几何意义是两个向量的相关度(或者也可以是一个矩阵投影到另一个维度上的值?). 也就是说, 我们得到了一个token和词典中另一个token的相关度
4. 这个时候根据我们的训练数据, 我们其实能够知道, 这个token的下一个token, 我们预期是什么样的, 也就是我们预期这个乘出来的值是什么, 这样根据损失函数, 就能反过去调整权重矩阵Wp. 怎么来理解这个调整, 我理解可以看成我们训练的过程, 就是在根据提供的训练数据(上下文)来微调这个128维向量的位置. 完成训练后, 这个token在空间中的位置就固定了, 也是我们期望的位置. 这个时候的Wp矩阵是符合我们期望的. 这个时候前面的[1, 128]的token, 乘以[128, vocab_size]的权重矩阵, 就能得到我们期望的下一个token的概率分布(前面训练的时候, 如果这里乘出来不符合我们期望, 就通过损失函数来调整Wp, 使得乘出来的值更接近我们期望的值).
5. 这样拿到上下文最后一个token的概率分布后, 就能根据概率分布来预测下一个token是什么, 这样就预测出下一个文字
6. 然后我们拿着下一个文字, 继续输入到模型中, 这样就能不断的预测下一个文字, 直到预测出结束符号

### reference
- [【17】Transformer最终输出逻辑及参数量](https://www.bilibili.com/video/BV1mm4116794)  
