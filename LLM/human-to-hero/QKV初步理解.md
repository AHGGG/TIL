### QKV初步理解

Q@K之后, 其实就是向前计算每个token之间的相关度, 值越大, 相关度越高
1. Q@K = [16, 16]

2. V = [16, 128]

3. 前面两个矩阵相乘, 得到A = 1 @ 2 = [16, 128] 

理解: 比如1矩阵的最后一行[1, 16] @ 2矩阵的第一列[16, 1], 可以理解成:
- 2矩阵V的第一列[16, 1], 其中每个值都代表每个token在某一个维度(方向/角度)的分量. 比第一行第一列, 就代表第一个token, 在方向1的分量
- 1矩阵的第一个元素(最后一行[1, 16]的第一个数值), 代表了最后一个token相对于第一个token的相关度的大小, 值越大相关度越高, 值越小相关度越小. 
- 这个时候用最后一个token相对于第一个token的相关程度, 去乘以第一个token在方向1的分量, 就得到了最后一个token在方向1的相关性
- 然后继续前面的循环, **最后一个token相当于第二个token的相关程度** ⅹ **第二个token在方向1的分量**, 得到了最后一个token在方向2的相关性

继续理解:
相当于最后一个token, 根据和前面每个token的相关程度, 分别乘以每个token在方向1的分量大小. 得到了最后一个token在方向1和每个token的相关性的数值
> 一个token是一个多维向量, 可以拆分为每个维度有一个向量+长度, 最后组成的这个多维向量. 我们因为计算了这个token和另一个token的相关程度(一个数值), 这样我们就能知道在同一个多维空间中, 它们每个维度的相关程度(通过相关度 ⅹ token在这个维度的长度, 来得到)

所以我们现在已知最后一个token, 相对于其他token的相关程度. 还知道了目前其他token在方向1上的长度值, 所以通过最后一个token和其他token的相关程度 ⅹ 这些token在方向1的长度值, 就能得到最后一个token相对于其他token的相关性(一个数值来表示). 

然后把这些数值加到一起, 就等于A矩阵最后一行, 第一列的数值. 
> 具体来说, 最后一个token相对于其他token在方向1的相似性的大小, 现在是A矩阵的最后一行, 第一列, 是一个数值. 也就是说, 可能和某个token非常相关, 所以相似性大, 跟某个token又不相关, 相似性又小甚至是负数. 最后这些和其他token的所有的相似性的值相加, 就是最后一个token的在方向1的相似度总和(因为一个+可能会被另一个-给综合掉, 所以这里求和后, 就是一个综合的数值, 代表: 最后一个token在方向1, 和其他token综合相似性)  
> 就是最后一个token, 对于其他所有文字, 在128个维度中, 第一个维度的权重综合

所以A矩阵的, 第一列的每个值, 都是当前文字的token对于其他token在维度1的关注度的权重大小总和

128个维度也相当于, 我们从128个特征空间的**某个特征的角度**来看, **某个token相对于其他token的相似性大小(一个数值表示)**

然后通过不断的训练, Wv权重矩阵不断更新, 让我们Q@K计算出相似程度 ⅹ 更新后的V矩阵后, 得到的A矩阵里的, 每个维度下, 每个token对于其他token的相似度值更准确. 这样一个context中, 一个token就能更好的理解在上下文中的意思

### reference
- [LLM张老师系列视频-QKV](https://www.bilibili.com/video/BV1Zu4m1u75U)  
> 这个up讲的真的很好, 之前看了一圈, 都没有提到矩阵乘法的几何意义(为什么两个向量相乘能计算出相关度), 然后看到这个这个宝藏up!!! 讲的话也控制在大白话, 知道一些概念和名字会带来干扰(会教), 推荐!